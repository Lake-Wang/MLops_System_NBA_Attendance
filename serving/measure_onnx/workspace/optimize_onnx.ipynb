{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "base_data_dir = os.getenv(\"NBA_DATA_DIR\", \"nba_data\")\n",
    "\n",
    "X1_train = pd.read_csv(os.path.join(base_data_dir, 'train/X_train_model1.csv'))\n",
    "X1_train = X1_train.drop('gameId', axis=1)\n",
    "X1_test = pd.read_csv(os.path.join(base_data_dir, 'test/X_test_model1.csv'))\n",
    "X1_test = X1_test.drop('gameId', axis=1)\n",
    "\n",
    "Y1_train = pd.read_csv(os.path.join(base_data_dir, 'train/Y_train_model1.csv'))\n",
    "Y1_test = pd.read_csv(os.path.join(base_data_dir, 'test/Y_test_model1.csv'))\n",
    "full1_df = pd.read_csv(os.path.join(base_data_dir, 'train/full_stats.csv'))\n",
    "\n",
    "X_save_cols = X1_train.columns\n",
    "\n",
    "# Convert to tensors, pass to dataloader\n",
    "X1_train = torch.tensor(X1_train.values, dtype=torch.float32)\n",
    "X1_test = torch.tensor(X1_test.values, dtype=torch.float32)\n",
    "Y1_train = torch.tensor(Y1_train.values, dtype=torch.float32)\n",
    "Y1_test = torch.tensor(Y1_test.values, dtype=torch.float32)\n",
    "\n",
    "train1_data = TensorDataset(X1_train, Y1_train)\n",
    "test1_data = TensorDataset(X1_test, Y1_test)\n",
    "\n",
    "train1_loader = DataLoader(train1_data, batch_size=32, shuffle=True)\n",
    "test1_loader = DataLoader(test1_data, batch_size=32, shuffle=False)\n",
    "\n",
    "game_ids = full1_df['gameId'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3906/1191081072.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1 = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/point_diff.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = torch.load(model_path, map_location=device)\n",
    "model1.eval()\n",
    "\n",
    "predictions = []\n",
    "extract_df = full1_df[X_save_cols].values\n",
    "X_tensor = torch.FloatTensor(extract_df).to(device)\n",
    "with torch.no_grad():\n",
    "    # Processsing in batches\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in range(0, len(X_tensor), batch_size):\n",
    "        batch = X_tensor[i:i+batch_size]\n",
    "\n",
    "        # FF, convert to numpy\n",
    "        batch_preds = model1(batch)\n",
    "        predictions.append(batch_preds.cpu().numpy())\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Return results\n",
    "result_df = pd.DataFrame({\n",
    "    'gameId': game_ids,\n",
    "    'predicted_point_diff': all_predictions.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X2_train = pd.read_csv(os.path.join(base_data_dir, 'train/X_train_model2.csv'))\n",
    "X2_test = pd.read_csv(os.path.join(base_data_dir, 'test/X_test_model2.csv'))\n",
    "Y2_train = pd.read_csv(os.path.join(base_data_dir, 'train/Y_train_model2.csv'))\n",
    "Y2_test = pd.read_csv(os.path.join(base_data_dir, 'test/Y_test_model2.csv'))\n",
    "\n",
    "X2_train = X2_train.merge(result_df, on='gameId', how='inner')\n",
    "X2_train = X2_train.drop('gameId', axis=1)\n",
    "X2_test = X2_test.merge(result_df, on='gameId', how='inner')\n",
    "X2_test = X2_test.drop('gameId', axis=1)\n",
    "full2_df = pd.read_csv(os.path.join(base_data_dir, 'train/full_attendance.csv'))\n",
    "full2_df = full2_df.merge(result_df, on='gameId', how='inner')\n",
    "\n",
    "X2_train = torch.tensor(X2_train.values, dtype=torch.float32)\n",
    "X2_test = torch.tensor(X2_test.values, dtype=torch.float32)\n",
    "Y2_train = torch.tensor(Y2_train.values, dtype=torch.float32)\n",
    "Y2_test = torch.tensor(Y2_test.values, dtype=torch.float32)\n",
    "\n",
    "train2_data = TensorDataset(X2_train, Y2_train)\n",
    "test2_data = TensorDataset(X2_test, Y2_test)\n",
    "train2_loader = DataLoader(train2_data, batch_size=32, shuffle=True)\n",
    "test2_loader = DataLoader(test2_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    total_mse = 0\n",
    "    total = 0\n",
    "    for features, labels in test2_loader:\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: features.numpy()})[0]\n",
    "        mse = ((outputs - labels.numpy()) ** 2).sum()\n",
    "        total_mse += mse\n",
    "        total += labels.size(0)\n",
    "    print(f\"Mean Absolute Error (MAE): {total_mse / total:.2f}\")\n",
    "\n",
    "    num_trials = 100\n",
    "    single_sample = X2_test[0].unsqueeze(0).numpy()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start)\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput: {num_trials / np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    num_batches = 50\n",
    "    batch_input = X2_test[:32].numpy()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start)\n",
    "    print(f\"Batch Throughput: {(batch_input.shape[0] * num_batches) / np.sum(batch_times):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model2_path = \"models/model2.onnx\"\n",
    "optimized_model2_path = \"models/model2_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "session_options.optimized_model_filepath = optimized_model2_path\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model2_path, sess_options=session_options, providers=['CPUExecutionProvider'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution provider: ['CPUExecutionProvider']\n",
      "Mean Absolute Error (MAE): 1363096.76\n",
      "Inference Latency (median): 0.11 ms\n",
      "Inference Throughput: 8211.08 FPS\n",
      "Batch Throughput: 60010.79 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/model2_optimized.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 00:27:37 [INFO] Start auto tuning.\n",
      "2025-05-12 00:27:37 [INFO] Quantize model without tuning!\n",
      "2025-05-12 00:27:37 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-12 00:27:38 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-12 00:27:38 [INFO] 0 recipes specified by user.\n",
      "2025-05-12 00:27:38 [INFO] 3 recipes require future tuning.\n",
      "2025-05-12 00:27:38 [INFO] *** Initialize auto tuning\n",
      "2025-05-12 00:27:38 [INFO] {\n",
      "2025-05-12 00:27:38 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-12 00:27:38 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-12 00:27:38 [INFO]             'criterion': 'relative',\n",
      "2025-05-12 00:27:38 [INFO]             'higher_is_better': True,\n",
      "2025-05-12 00:27:38 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-12 00:27:38 [INFO]             'absolute': None,\n",
      "2025-05-12 00:27:38 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7ac26162b500>>,\n",
      "2025-05-12 00:27:38 [INFO]             'relative': 0.01\n",
      "2025-05-12 00:27:38 [INFO]         },\n",
      "2025-05-12 00:27:38 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-12 00:27:38 [INFO]         'backend': 'default',\n",
      "2025-05-12 00:27:38 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-12 00:27:38 [INFO]             100\n",
      "2025-05-12 00:27:38 [INFO]         ],\n",
      "2025-05-12 00:27:38 [INFO]         'device': 'cpu',\n",
      "2025-05-12 00:27:38 [INFO]         'domain': 'auto',\n",
      "2025-05-12 00:27:38 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-12 00:27:38 [INFO]         'excluded_precisions': [\n",
      "2025-05-12 00:27:38 [INFO]         ],\n",
      "2025-05-12 00:27:38 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-12 00:27:38 [INFO]         'inputs': [\n",
      "2025-05-12 00:27:38 [INFO]         ],\n",
      "2025-05-12 00:27:38 [INFO]         'model_name': '',\n",
      "2025-05-12 00:27:38 [INFO]         'op_name_dict': None,\n",
      "2025-05-12 00:27:38 [INFO]         'op_type_dict': None,\n",
      "2025-05-12 00:27:38 [INFO]         'outputs': [\n",
      "2025-05-12 00:27:38 [INFO]         ],\n",
      "2025-05-12 00:27:38 [INFO]         'quant_format': 'default',\n",
      "2025-05-12 00:27:38 [INFO]         'quant_level': 'auto',\n",
      "2025-05-12 00:27:38 [INFO]         'recipes': {\n",
      "2025-05-12 00:27:38 [INFO]             'smooth_quant': False,\n",
      "2025-05-12 00:27:38 [INFO]             'smooth_quant_args': {\n",
      "2025-05-12 00:27:38 [INFO]             },\n",
      "2025-05-12 00:27:38 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-12 00:27:38 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-12 00:27:38 [INFO]             },\n",
      "2025-05-12 00:27:38 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-12 00:27:38 [INFO]             'weight_correction': False,\n",
      "2025-05-12 00:27:38 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-12 00:27:38 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-12 00:27:38 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-12 00:27:38 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-12 00:27:38 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-12 00:27:38 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-12 00:27:38 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-12 00:27:38 [INFO]             ],\n",
      "2025-05-12 00:27:38 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-12 00:27:38 [INFO]             'rtn_args': {\n",
      "2025-05-12 00:27:38 [INFO]             },\n",
      "2025-05-12 00:27:38 [INFO]             'awq_args': {\n",
      "2025-05-12 00:27:38 [INFO]             },\n",
      "2025-05-12 00:27:38 [INFO]             'gptq_args': {\n",
      "2025-05-12 00:27:38 [INFO]             },\n",
      "2025-05-12 00:27:38 [INFO]             'teq_args': {\n",
      "2025-05-12 00:27:38 [INFO]             },\n",
      "2025-05-12 00:27:38 [INFO]             'autoround_args': {\n",
      "2025-05-12 00:27:38 [INFO]             }\n",
      "2025-05-12 00:27:38 [INFO]         },\n",
      "2025-05-12 00:27:38 [INFO]         'reduce_range': None,\n",
      "2025-05-12 00:27:38 [INFO]         'TuningCriterion': {\n",
      "2025-05-12 00:27:38 [INFO]             'max_trials': 100,\n",
      "2025-05-12 00:27:38 [INFO]             'objective': [\n",
      "2025-05-12 00:27:38 [INFO]                 'performance'\n",
      "2025-05-12 00:27:38 [INFO]             ],\n",
      "2025-05-12 00:27:38 [INFO]             'strategy': 'basic',\n",
      "2025-05-12 00:27:38 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-12 00:27:38 [INFO]             'timeout': 0\n",
      "2025-05-12 00:27:38 [INFO]         },\n",
      "2025-05-12 00:27:38 [INFO]         'use_bf16': True,\n",
      "2025-05-12 00:27:38 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-12 00:27:38 [INFO]     }\n",
      "2025-05-12 00:27:38 [INFO] }\n",
      "2025-05-12 00:27:38 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-12 00:27:38 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-12 00:27:38 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-05-12 00:27:38 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-12 00:27:38 [INFO] Quantize the model with default config.\n",
      "2025-05-12 00:27:38 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-12 00:27:38 [INFO] +-----------------------+-------+------+\n",
      "2025-05-12 00:27:38 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-12 00:27:38 [INFO] +-----------------------+-------+------+\n",
      "2025-05-12 00:27:38 [INFO] |         MatMul        |   5   |  5   |\n",
      "2025-05-12 00:27:38 [INFO] | DynamicQuantizeLinear |   5   |  5   |\n",
      "2025-05-12 00:27:38 [INFO] +-----------------------+-------+------+\n",
      "2025-05-12 00:27:38 [INFO] Pass quantize model elapsed time: 101.23 ms\n",
      "2025-05-12 00:27:38 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/./history.snapshot.\n",
      "2025-05-12 00:27:38 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-12 00:27:38 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-12 00:27:38 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/model2.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(approach=\"dynamic\")\n",
    "\n",
    "q_model = quantization.fit(model=fp32_model, conf=config_ptq)\n",
    "\n",
    "q_model.save_model_to_file(\"models/model2_quantized_dynamic.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 0.75 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "Mean Absolute Error (MAE): 1367935.12\n",
      "Inference Latency (median): 0.09 ms\n",
      "Inference Throughput: 11230.03 FPS\n",
      "Batch Throughput: 58410.90 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/model2_quantized_dynamic.onnx\"\n",
    "print(f\"Model Size on Disk: {os.path.getsize(onnx_model_path) / 1e6:.2f} MB\")\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 00:55:03 [INFO] Start basic tuning.\n",
      "2025-05-12 00:55:03 [INFO] Create evaluation function according to evaluation dataloader and metric                and Execute the tuning process.\n",
      "2025-05-12 00:55:03 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-12 00:55:03 [INFO] 0 recipes specified by user.\n",
      "2025-05-12 00:55:03 [INFO] 3 recipes require future tuning.\n",
      "2025-05-12 00:55:03 [INFO] {\n",
      "2025-05-12 00:55:03 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-12 00:55:03 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-12 00:55:03 [INFO]             'criterion': 'relative',\n",
      "2025-05-12 00:55:03 [INFO]             'higher_is_better': True,\n",
      "2025-05-12 00:55:03 [INFO]             'tolerable_loss': 0.1,\n",
      "2025-05-12 00:55:03 [INFO]             'absolute': None,\n",
      "2025-05-12 00:55:03 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7ac254559d30>>,\n",
      "2025-05-12 00:55:03 [INFO]             'relative': 0.1\n",
      "2025-05-12 00:55:03 [INFO]         },\n",
      "2025-05-12 00:55:03 [INFO]         'approach': 'post_training_static_quant',\n",
      "2025-05-12 00:55:03 [INFO]         'backend': 'default',\n",
      "2025-05-12 00:55:03 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-12 00:55:03 [INFO]             128\n",
      "2025-05-12 00:55:03 [INFO]         ],\n",
      "2025-05-12 00:55:03 [INFO]         'device': 'cpu',\n",
      "2025-05-12 00:55:03 [INFO]         'domain': 'auto',\n",
      "2025-05-12 00:55:03 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-12 00:55:03 [INFO]         'excluded_precisions': [\n",
      "2025-05-12 00:55:03 [INFO]         ],\n",
      "2025-05-12 00:55:03 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-12 00:55:03 [INFO]         'inputs': [\n",
      "2025-05-12 00:55:03 [INFO]         ],\n",
      "2025-05-12 00:55:03 [INFO]         'model_name': '',\n",
      "2025-05-12 00:55:03 [INFO]         'op_name_dict': None,\n",
      "2025-05-12 00:55:03 [INFO]         'op_type_dict': None,\n",
      "2025-05-12 00:55:03 [INFO]         'outputs': [\n",
      "2025-05-12 00:55:03 [INFO]         ],\n",
      "2025-05-12 00:55:03 [INFO]         'quant_format': 'QOperator',\n",
      "2025-05-12 00:55:03 [INFO]         'quant_level': 1,\n",
      "2025-05-12 00:55:03 [INFO]         'recipes': {\n",
      "2025-05-12 00:55:03 [INFO]             'smooth_quant': False,\n",
      "2025-05-12 00:55:03 [INFO]             'smooth_quant_args': {\n",
      "2025-05-12 00:55:03 [INFO]             },\n",
      "2025-05-12 00:55:03 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-12 00:55:03 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-12 00:55:03 [INFO]             },\n",
      "2025-05-12 00:55:03 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-12 00:55:03 [INFO]             'weight_correction': False,\n",
      "2025-05-12 00:55:03 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-12 00:55:03 [INFO]             'graph_optimization_level': 'ENABLE_EXTENDED',\n",
      "2025-05-12 00:55:03 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-12 00:55:03 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-12 00:55:03 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-12 00:55:03 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-12 00:55:03 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-12 00:55:03 [INFO]             ],\n",
      "2025-05-12 00:55:03 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-12 00:55:03 [INFO]             'rtn_args': {\n",
      "2025-05-12 00:55:03 [INFO]             },\n",
      "2025-05-12 00:55:03 [INFO]             'awq_args': {\n",
      "2025-05-12 00:55:03 [INFO]             },\n",
      "2025-05-12 00:55:03 [INFO]             'gptq_args': {\n",
      "2025-05-12 00:55:03 [INFO]             },\n",
      "2025-05-12 00:55:03 [INFO]             'teq_args': {\n",
      "2025-05-12 00:55:03 [INFO]             },\n",
      "2025-05-12 00:55:03 [INFO]             'autoround_args': {\n",
      "2025-05-12 00:55:03 [INFO]             }\n",
      "2025-05-12 00:55:03 [INFO]         },\n",
      "2025-05-12 00:55:03 [INFO]         'reduce_range': None,\n",
      "2025-05-12 00:55:03 [INFO]         'TuningCriterion': {\n",
      "2025-05-12 00:55:03 [INFO]             'max_trials': 100,\n",
      "2025-05-12 00:55:03 [INFO]             'objective': [\n",
      "2025-05-12 00:55:03 [INFO]                 'performance'\n",
      "2025-05-12 00:55:03 [INFO]             ],\n",
      "2025-05-12 00:55:03 [INFO]             'strategy': 'basic',\n",
      "2025-05-12 00:55:03 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-12 00:55:03 [INFO]             'timeout': 0\n",
      "2025-05-12 00:55:03 [INFO]         },\n",
      "2025-05-12 00:55:03 [INFO]         'use_bf16': True,\n",
      "2025-05-12 00:55:03 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-12 00:55:03 [INFO]     }\n",
      "2025-05-12 00:55:03 [INFO] }\n",
      "2025-05-12 00:55:03 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-12 00:55:03 [INFO] Get FP32 model baseline.\n",
      "2025-05-12 00:55:03 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/./history.snapshot.\n",
      "2025-05-12 00:55:03 [INFO] FP32 baseline is: [Accuracy: 1167.5174, Duration (seconds): 0.2813]\n",
      "2025-05-12 00:55:03 [INFO] |*******Mixed Precision Statistics*******|\n",
      "2025-05-12 00:55:03 [INFO] +------------------+-------+------+------+\n",
      "2025-05-12 00:55:03 [INFO] |     Op Type      | Total | INT8 | FP32 |\n",
      "2025-05-12 00:55:03 [INFO] +------------------+-------+------+------+\n",
      "2025-05-12 00:55:03 [INFO] |      MatMul      |   1   |  1   |  0   |\n",
      "2025-05-12 00:55:03 [INFO] |       Clip       |   1   |  0   |  1   |\n",
      "2025-05-12 00:55:03 [INFO] |       Add        |   1   |  1   |  0   |\n",
      "2025-05-12 00:55:03 [INFO] |      Expand      |   1   |  0   |  1   |\n",
      "2025-05-12 00:55:03 [INFO] |  QuantizeLinear  |   1   |  1   |  0   |\n",
      "2025-05-12 00:55:03 [INFO] | DequantizeLinear |   1   |  1   |  0   |\n",
      "2025-05-12 00:55:03 [INFO] +------------------+-------+------+------+\n",
      "2025-05-12 00:55:03 [INFO] Pass quantize model elapsed time: 84.91 ms\n",
      "2025-05-12 00:55:03 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1168.6801|1167.5174, Duration (seconds) (int8|fp32): 0.2949|0.2813], Best tune result is: [Accuracy: 1168.6801, Duration (seconds): 0.2949]\n",
      "2025-05-12 00:55:03 [INFO] |***********************Tune Result Statistics***********************|\n",
      "2025-05-12 00:55:03 [INFO] +--------------------+------------+---------------+------------------+\n",
      "2025-05-12 00:55:03 [INFO] |     Info Type      |  Baseline  | Tune 1 result | Best tune result |\n",
      "2025-05-12 00:55:03 [INFO] +--------------------+------------+---------------+------------------+\n",
      "2025-05-12 00:55:03 [INFO] |      Accuracy      | 1167.5174  |   1168.6801   |    1168.6801     |\n",
      "2025-05-12 00:55:03 [INFO] | Duration (seconds) |  0.2813    |    0.2949     |     0.2949       |\n",
      "2025-05-12 00:55:03 [INFO] +--------------------+------------+---------------+------------------+\n",
      "2025-05-12 00:55:03 [INFO] [Strategy] Found a model that meets the accuracy requirements.\n",
      "2025-05-12 00:55:03 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/./history.snapshot.\n",
      "2025-05-12 00:55:03 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-12 00:55:03 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "from neural_compressor.data import DataLoader\n",
    "from neural_compressor import PostTrainingQuantConfig\n",
    "from neural_compressor.config import AccuracyCriterion\n",
    "from neural_compressor.metric import Metric\n",
    "\n",
    "model_path = \"models/model2.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    framework='onnxruntime',\n",
    "    dataset=test2_data  # regression: X and y paired\n",
    ")\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = PostTrainingQuantConfig(\n",
    "    accuracy_criterion=AccuracyCriterion(\n",
    "        criterion=\"relative\",  \n",
    "        tolerable_loss=0.1  # tolerance for regression metric loss (e.g., MSE increase)\n",
    "    ),\n",
    "    approach=\"static\", \n",
    "    device='cpu', \n",
    "    quant_level=1,\n",
    "    quant_format=\"QOperator\", \n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"}, \n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "# Use built-in regression metric or a custom eval_func\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq, \n",
    "    calib_dataloader=eval_dataloader,\n",
    "    eval_dataloader=eval_dataloader, \n",
    "    eval_metric=neural_compressor.metric.Metric(name='RMSE')\n",
    ")\n",
    "\n",
    "\n",
    "q_model.save_model_to_file(\"models/model2_quantized_aggressive.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 2.93 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "Mean Absolute Error (MAE): 1365813.24\n",
      "Inference Latency (median): 0.13 ms\n",
      "Inference Throughput: 7724.89 FPS\n",
      "Batch Throughput: 60007.03 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/model2_quantized_aggressive.onnx\"\n",
    "print(f\"Model Size on Disk: {os.path.getsize(onnx_model_path) / 1e6:.2f} MB\")\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 00:58:20 [INFO] Start conservative tuning.\n",
      "2025-05-12 00:58:20 [INFO] Create evaluation function according to evaluation dataloader and metric                and Execute the tuning process.\n",
      "2025-05-12 00:58:20 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-12 00:58:20 [INFO] 0 recipes specified by user.\n",
      "2025-05-12 00:58:20 [INFO] 3 recipes require future tuning.\n",
      "2025-05-12 00:58:20 [INFO] *** Initialize conservative tuning\n",
      "2025-05-12 00:58:20 [INFO] {\n",
      "2025-05-12 00:58:20 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-12 00:58:20 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-12 00:58:20 [INFO]             'criterion': 'relative',\n",
      "2025-05-12 00:58:20 [INFO]             'higher_is_better': True,\n",
      "2025-05-12 00:58:20 [INFO]             'tolerable_loss': 0.1,\n",
      "2025-05-12 00:58:20 [INFO]             'absolute': None,\n",
      "2025-05-12 00:58:20 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7ac25758f7d0>>,\n",
      "2025-05-12 00:58:20 [INFO]             'relative': 0.1\n",
      "2025-05-12 00:58:20 [INFO]         },\n",
      "2025-05-12 00:58:20 [INFO]         'approach': 'post_training_static_quant',\n",
      "2025-05-12 00:58:20 [INFO]         'backend': 'default',\n",
      "2025-05-12 00:58:20 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-12 00:58:20 [INFO]             128\n",
      "2025-05-12 00:58:20 [INFO]         ],\n",
      "2025-05-12 00:58:20 [INFO]         'device': 'cpu',\n",
      "2025-05-12 00:58:20 [INFO]         'domain': 'auto',\n",
      "2025-05-12 00:58:20 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-12 00:58:20 [INFO]         'excluded_precisions': [\n",
      "2025-05-12 00:58:20 [INFO]         ],\n",
      "2025-05-12 00:58:20 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-12 00:58:20 [INFO]         'inputs': [\n",
      "2025-05-12 00:58:20 [INFO]         ],\n",
      "2025-05-12 00:58:20 [INFO]         'model_name': '',\n",
      "2025-05-12 00:58:20 [INFO]         'op_name_dict': None,\n",
      "2025-05-12 00:58:20 [INFO]         'op_type_dict': None,\n",
      "2025-05-12 00:58:20 [INFO]         'outputs': [\n",
      "2025-05-12 00:58:20 [INFO]         ],\n",
      "2025-05-12 00:58:20 [INFO]         'quant_format': 'QOperator',\n",
      "2025-05-12 00:58:20 [INFO]         'quant_level': 0,\n",
      "2025-05-12 00:58:20 [INFO]         'recipes': {\n",
      "2025-05-12 00:58:20 [INFO]             'smooth_quant': False,\n",
      "2025-05-12 00:58:20 [INFO]             'smooth_quant_args': {\n",
      "2025-05-12 00:58:20 [INFO]             },\n",
      "2025-05-12 00:58:20 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-12 00:58:20 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-12 00:58:20 [INFO]             },\n",
      "2025-05-12 00:58:20 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-12 00:58:20 [INFO]             'weight_correction': False,\n",
      "2025-05-12 00:58:20 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-12 00:58:20 [INFO]             'graph_optimization_level': 'ENABLE_EXTENDED',\n",
      "2025-05-12 00:58:20 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-12 00:58:20 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-12 00:58:20 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-12 00:58:20 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-12 00:58:20 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-12 00:58:20 [INFO]             ],\n",
      "2025-05-12 00:58:20 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-12 00:58:20 [INFO]             'rtn_args': {\n",
      "2025-05-12 00:58:20 [INFO]             },\n",
      "2025-05-12 00:58:20 [INFO]             'awq_args': {\n",
      "2025-05-12 00:58:20 [INFO]             },\n",
      "2025-05-12 00:58:20 [INFO]             'gptq_args': {\n",
      "2025-05-12 00:58:20 [INFO]             },\n",
      "2025-05-12 00:58:20 [INFO]             'teq_args': {\n",
      "2025-05-12 00:58:20 [INFO]             },\n",
      "2025-05-12 00:58:20 [INFO]             'autoround_args': {\n",
      "2025-05-12 00:58:20 [INFO]             }\n",
      "2025-05-12 00:58:20 [INFO]         },\n",
      "2025-05-12 00:58:20 [INFO]         'reduce_range': None,\n",
      "2025-05-12 00:58:20 [INFO]         'TuningCriterion': {\n",
      "2025-05-12 00:58:20 [INFO]             'max_trials': 100,\n",
      "2025-05-12 00:58:20 [INFO]             'objective': [\n",
      "2025-05-12 00:58:20 [INFO]                 'performance'\n",
      "2025-05-12 00:58:20 [INFO]             ],\n",
      "2025-05-12 00:58:20 [INFO]             'strategy': 'basic',\n",
      "2025-05-12 00:58:20 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-12 00:58:20 [INFO]             'timeout': 0\n",
      "2025-05-12 00:58:20 [INFO]         },\n",
      "2025-05-12 00:58:20 [INFO]         'use_bf16': True,\n",
      "2025-05-12 00:58:20 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-12 00:58:20 [INFO]     }\n",
      "2025-05-12 00:58:20 [INFO] }\n",
      "2025-05-12 00:58:20 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-12 00:58:20 [INFO] Get FP32 model baseline.\n",
      "2025-05-12 00:58:20 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/./history.snapshot.\n",
      "2025-05-12 00:58:20 [INFO] FP32 baseline is: [Accuracy: 1167.5174, Duration (seconds): 0.2854]\n",
      "2025-05-12 00:58:20 [INFO] *** Try to convert op into lower precision to improve performance.\n",
      "2025-05-12 00:58:20 [INFO] *** Start to convert op into int8.\n",
      "2025-05-12 00:58:20 [INFO] *** Try to convert all matmul ops into int8.\n",
      "2025-05-12 00:58:20 [INFO] |*******Mixed Precision Statistics*******|\n",
      "2025-05-12 00:58:20 [INFO] +------------------+-------+------+------+\n",
      "2025-05-12 00:58:20 [INFO] |     Op Type      | Total | INT8 | FP32 |\n",
      "2025-05-12 00:58:20 [INFO] +------------------+-------+------+------+\n",
      "2025-05-12 00:58:20 [INFO] |      MatMul      |   1   |  1   |  0   |\n",
      "2025-05-12 00:58:20 [INFO] |       Clip       |   1   |  0   |  1   |\n",
      "2025-05-12 00:58:20 [INFO] |       Add        |   1   |  0   |  1   |\n",
      "2025-05-12 00:58:20 [INFO] |      Expand      |   1   |  0   |  1   |\n",
      "2025-05-12 00:58:20 [INFO] |  QuantizeLinear  |   1   |  1   |  0   |\n",
      "2025-05-12 00:58:20 [INFO] | DequantizeLinear |   1   |  1   |  0   |\n",
      "2025-05-12 00:58:20 [INFO] +------------------+-------+------+------+\n",
      "2025-05-12 00:58:20 [INFO] Pass quantize model elapsed time: 73.16 ms\n",
      "2025-05-12 00:58:20 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1168.6793|1167.5174, Duration (seconds) (int8|fp32): 0.3196|0.2854], Best tune result is: [Accuracy: 1168.6793, Duration (seconds): 0.3196]\n",
      "2025-05-12 00:58:20 [INFO] |***********************Tune Result Statistics***********************|\n",
      "2025-05-12 00:58:20 [INFO] +--------------------+------------+---------------+------------------+\n",
      "2025-05-12 00:58:20 [INFO] |     Info Type      |  Baseline  | Tune 1 result | Best tune result |\n",
      "2025-05-12 00:58:20 [INFO] +--------------------+------------+---------------+------------------+\n",
      "2025-05-12 00:58:20 [INFO] |      Accuracy      | 1167.5174  |   1168.6793   |    1168.6793     |\n",
      "2025-05-12 00:58:20 [INFO] | Duration (seconds) |  0.2854    |    0.3196     |     0.3196       |\n",
      "2025-05-12 00:58:20 [INFO] +--------------------+------------+---------------+------------------+\n",
      "2025-05-12 00:58:20 [INFO] [Strategy] Found a model that meets the accuracy requirements.\n",
      "2025-05-12 00:58:20 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/./history.snapshot.\n",
      "2025-05-12 00:58:20 [INFO] *** Do not stop the tuning process, re-quantize the ops.\n",
      "2025-05-12 00:58:20 [INFO] *** Convert all matmul ops to int8 and accuracy still meet the requirements\n",
      "2025-05-12 00:58:20 [INFO] ***Current result dict_items([('conv', None), ('matmul', 'int8'), ('bmm', None), ('linear', None)])\n",
      "2025-05-12 00:58:20 [INFO] *** Ending tuning process due to no quantifiable op left.\n",
      "2025-05-12 00:58:20 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-12 00:58:20 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-12_00-24-02/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = \"models/model2.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "config_ptq = PostTrainingQuantConfig(\n",
    "    accuracy_criterion=AccuracyCriterion(\n",
    "        criterion=\"relative\",  \n",
    "        tolerable_loss=0.1  # tolerance for regression metric loss (e.g., MSE increase)\n",
    "    ),\n",
    "    approach=\"static\", \n",
    "    device='cpu', \n",
    "    quant_level=0,\n",
    "    quant_format=\"QOperator\", \n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"}, \n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq, \n",
    "    calib_dataloader=eval_dataloader,\n",
    "    eval_dataloader=eval_dataloader, \n",
    "    eval_metric=neural_compressor.metric.Metric(name='RMSE')\n",
    ")\n",
    "\n",
    "q_model.save_model_to_file(\"models/model2_quantized_conservative.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 2.93 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "Mean Absolute Error (MAE): 1365811.42\n",
      "Inference Latency (median): 0.13 ms\n",
      "Inference Throughput: 5138.06 FPS\n",
      "Batch Throughput: 35998.94 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/model2_quantized_conservative.onnx\"\n",
    "print(f\"Model Size on Disk: {os.path.getsize(onnx_model_path) / 1e6:.2f} MB\")\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
