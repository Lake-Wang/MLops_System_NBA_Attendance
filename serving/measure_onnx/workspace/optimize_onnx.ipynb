{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import neural_compressor\n",
    "from neural_compressor import quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "base_data_dir = os.getenv(\"NBA_DATA_DIR\", \"nba_data\")\n",
    "\n",
    "X1_train = pd.read_csv(os.path.join(base_data_dir, 'train/X_train_model1.csv'))\n",
    "X1_test = pd.read_csv(os.path.join(base_data_dir, 'train/X_test_model1.csv'))\n",
    "Y1_train = pd.read_csv(os.path.join(base_data_dir, 'train/Y_train_model1.csv'))\n",
    "Y1_test = pd.read_csv(os.path.join(base_data_dir, 'train/Y_test_model1.csv'))\n",
    "full1_df = pd.read_csv(os.path.join(base_data_dir, 'train/full_stats.csv'))\n",
    "\n",
    "# Convert to tensors, pass to dataloader\n",
    "X1_train = torch.tensor(X1_train.values, dtype=torch.float32)\n",
    "X1_test = torch.tensor(X1_test.values, dtype=torch.float32)\n",
    "Y1_train = torch.tensor(Y1_train.values, dtype=torch.float32)\n",
    "Y1_test = torch.tensor(Y1_test.values, dtype=torch.float32)\n",
    "\n",
    "train1_data = TensorDataset(X1_train, Y1_train)\n",
    "test1_data = TensorDataset(X1_test, Y1_test)\n",
    "\n",
    "train1_loader = DataLoader(train1_data, batch_size=32, shuffle=True)\n",
    "test1_loader = DataLoader(test1_data, batch_size=32, shuffle=False)\n",
    "\n",
    "game_ids = full1_df['gameId'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = \"models/point_diff.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = torch.load(model_path, map_location=device)\n",
    "model1.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    # Processsing in batches\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in range(0, len(X1_test), batch_size):\n",
    "        batch = X1_test[i:i+batch_size]\n",
    "\n",
    "        # FF, convert to numpy\n",
    "        batch_preds = model1(batch)\n",
    "        predictions.append(batch_preds.cpu().numpy())\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Return results\n",
    "result_df = pd.DataFrame({\n",
    "    'gameId': game_ids,\n",
    "    'predicted_point_diff': all_predictions.flatten()\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X2_train = pd.read_csv(os.path.join(base_data_dir, 'train/X_train_model2.csv'))\n",
    "X2_test = pd.read_csv(os.path.join(base_data_dir, 'test/X_test_model2.csv'))\n",
    "Y2_train = pd.read_csv(os.path.join(base_data_dir, 'train/Y_train_model2.csv'))\n",
    "Y2_test = pd.read_csv(os.path.join(base_data_dir, 'test/Y_test_model2.csv'))\n",
    "\n",
    "X2_train = X2_train.merge(result_df, on='gameId', how='inner')\n",
    "X2_train = X2_train.drop('gameId', axis=1)\n",
    "X2_test = X2_test.merge(result_df, on='gameId', how='inner')\n",
    "X2_test = X2_test.drop('gameId', axis=1)\n",
    "full2_df = pd.read_csv(os.path.join(base_data_dir, 'train/full_attendance.csv'))\n",
    "full2_df = full2_df.merge(result_df, on='gameId', how='inner')\n",
    "\n",
    "# train2_data = TensorDataset(X2_train, Y2_train)\n",
    "# test2_data = TensorDataset(X2_test, Y2_test)\n",
    "# train2_loader = DataLoader(train2_data, batch_size=32, shuffle=True)\n",
    "# test2_loader = DataLoader(test2_data, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    total_mse = 0\n",
    "    total = 0\n",
    "    for features, labels in test1_loader:\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: features.numpy()})[0]\n",
    "        mse = ((outputs - labels.numpy()) ** 2).sum()\n",
    "        total_mse += mse\n",
    "        total += labels.size(0)\n",
    "    print(f\"Mean Absolute Error (MAE): {total_mse / total:.2f}\")\n",
    "\n",
    "    num_trials = 100\n",
    "    single_sample = X2_test[0].unsqueeze(0).numpy()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start)\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput: {num_trials / np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    num_batches = 50\n",
    "    batch_input = X2_test[:32].numpy()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start)\n",
    "    print(f\"Batch Throughput: {(batch_input.shape[0] * num_batches) / np.sum(batch_times):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model2_path = \"models/mode2.onnx\"\n",
    "optimized_model2_path = \"models/model2_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "session_options.optimized_model_filepath = optimized_model2_path\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model2_path, sess_options=session_options, providers=['CPUExecutionProvider'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/model2_optimized.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/model2.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(approach=\"dynamic\")\n",
    "\n",
    "q_model = quantization.fit(model=fp32_model, conf=config_ptq)\n",
    "\n",
    "q_model.save_model_to_file(\"models/model2_quantized_dynamic.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/model2_quantized_dynamic.onnx\"\n",
    "print(f\"Model Size on Disk: {os.path.getsize(onnx_model_path) / 1e6:.2f} MB\")\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dataset = neural_compressor.data.datasets.NumpyDataset(X1_test.numpy())\n",
    "calib_dataloader = neural_compressor.data.DataLoader(framework='onnxruntime', dataset=calib_dataset)\n",
    "\n",
    "config_static_aggressive = neural_compressor.PostTrainingQuantConfig(\n",
    "    accuracy_criterion=neural_compressor.config.AccuracyCriterion(criterion=\"absolute\", tolerable_loss=0.05),\n",
    "    approach=\"static\",\n",
    "    device='cpu',\n",
    "    quant_level=1,\n",
    "    quant_format=\"QOperator\",\n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"},\n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model,\n",
    "    conf=config_static_aggressive,\n",
    "    calib_dataloader=calib_dataloader,\n",
    "    eval_dataloader=calib_dataloader\n",
    ")\n",
    "\n",
    "q_model.save_model_to_file(\"models/model2_quantized_aggressive.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/model2_quantized_aggressive.onnx\"\n",
    "print(f\"Model Size on Disk: {os.path.getsize(onnx_model_path) / 1e6:.2f} MB\")\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_static_conservative = neural_compressor.PostTrainingQuantConfig(\n",
    "    accuracy_criterion=neural_compressor.config.AccuracyCriterion(criterion=\"absolute\", tolerable_loss=0.01),\n",
    "    approach=\"static\",\n",
    "    device='cpu',\n",
    "    quant_level=0,\n",
    "    quant_format=\"QOperator\",\n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"},\n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model,\n",
    "    conf=config_static_conservative,\n",
    "    calib_dataloader=calib_dataloader,\n",
    "    eval_dataloader=calib_dataloader\n",
    ")\n",
    "\n",
    "q_model.save_model_to_file(\"models/model2_quantized_conservative.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/model2_quantized_conservative.onnx\"\n",
    "print(f\"Model Size on Disk: {os.path.getsize(onnx_model_path) / 1e6:.2f} MB\")\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
